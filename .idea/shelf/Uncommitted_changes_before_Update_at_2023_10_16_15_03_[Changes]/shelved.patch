Index: CEEMDAN-DBO-VMD-DBO-Informer-00.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>#!/usr/bin/env python\r\n# coding: utf-8\r\n\r\n# In[1]:\r\n\r\n\r\nimport os                           # 导入os模块用于操作系统相关函数\r\nimport copy                         # 导入copy模块用于创建对象的副本\r\nimport numpy                        # 导入numpy库进行数值操作\r\nimport random                       # 导入random模块用于生成随机数\r\nimport datetime                     # 导入datetime模块用于处理日期和时间\r\nimport math                         # 导入math模块进行数学运算\r\nimport pandas as pd                 # 导入pandas库进行数据操作和分析\r\nimport numpy as np                  # 导入numpy并使用别名\"np\"以方便使用\r\nimport matplotlib.pyplot as plt    # 导入matplotlib库进行数据可视化操作\r\nfrom PyEMD import CEEMDAN           # 从PyEMD库导入CEEMDAN模块\r\nfrom pyroapi import optim\r\nfrom sampen import sampen2          # 从sampen库导入sampen2模块\r\nfrom vmdpy import VMD               # 从vmdpy库导入VMD模块\r\nimport tensorflow as tf             # 导入tensorflow库进行机器学习和深度学习\r\nfrom sklearn.cluster import KMeans  # 从sklearn.cluster库导入KMeans模块\r\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error  # 导入评估指标函数\r\nfrom sklearn.preprocessing import MinMaxScaler    # 从sklearn.preprocessing库导入MinMaxScaler模块用于特征缩放\r\nimport warnings                    # 导入warnings库用于忽略警告\r\nfrom scipy.fftpack import hilbert, fft, ifft  # 导入傅里叶变换相关函数\r\nfrom math import log                # 导入log函数\r\nfrom typing import List             # 导入List类型用于类型提示\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.utils.data import DataLoader, Dataset\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom tqdm import tqdm\r\n\r\nfrom data.dataset import MyDataset\r\nfrom model.informer import Informer\r\nfrom utils.setseed import set_seed\r\n\r\nwarnings.filterwarnings(\"ignore\")    # 忽略警告消息\r\n\r\n\r\n# In[3]:\r\n\r\n\r\n# 用来正常显示中文标签\r\nplt.rcParams['font.sans-serif'] = ['SimHei']\r\n# 用来正常显示负号\r\nplt.rcParams['axes.unicode_minus'] = False\r\n\r\n\r\n# In[4]:\r\n\r\n\r\n# df_raw_data = pd.read_csv('焦作.csv', usecols=[0, 1])  # 从名为'焦作.csv'的CSV文件中读取数据，只使用第一列和第二列的数据创建DataFrame对象\r\ndf_raw_data = pd.read_csv(\"./data/ETT/ETTh1.csv\")\r\nX = 'OT'  # 将字符串'AQI'赋值给变量X，表示使用该列作为特征\r\n#\r\nseries_close = pd.Series(df_raw_data[X].values, index=df_raw_data['date'])  # 使用列名为X的数据创建Series对象，使用'Date'列作为索引\r\n#\r\ntest = df_raw_data[X].values[int(len(df_raw_data[X].values)*0.7):]  # 从X列的数据中提取后80%部分，并将结果存储在test变量中\r\n#\r\n\r\n# In[5]:\r\n\r\n\r\ntimestep = 30  # 定义时间步数，用于创建训练集和测试集\r\ntau = 0.  # 设置VMD分解的参数tau，用于控制模态函数的带宽\r\nDC = 0  # 设置VMD分解的参数DC，用于控制是否提取直流分量\r\ninit = 1  # 设置VMD分解的参数init，用于设置初始模态函数数量\r\ntol = 1e-7  # 设置VMD分解的参数tol，用于控制停止迭代的阈值\r\n\r\n\r\n# In[6]:\r\n\r\n\r\n''' 种群初始化函数 '''\r\ndef initial(pop, dim, ub, lb):\r\n    X = np.zeros([pop, dim])  # 创建一个形状为 (pop, dim) 的全零数组 X，用于存储种群的初始位置\r\n    for i in range(pop):\r\n        for j in range(dim):\r\n            X[i, j] = random.random() * (ub[j] - lb[j]) + lb[j]  # 生成一个位于 lb[j] 和 ub[j] 之间的随机数，赋值给 X[i, j]\r\n    return X, lb, ub\r\n\r\n'''边界检查函数'''\r\ndef BorderCheck(X, ub, lb, pop, dim):\r\n    for i in range(pop):\r\n        for j in range(dim):\r\n            if X[i, j] > ub[j]:  # 如果 X[i, j] 大于上界 ub[j]，将其设置为上界\r\n                X[i, j] = ub[j]\r\n            elif X[i, j] < lb[j]:  # 如果 X[i, j] 小于下界 lb[j]，将其设置为下界\r\n                X[i, j] = lb[j]\r\n    return X\r\n\r\n'''计算适应度函数'''\r\ndef CaculateFitness(X, fun):\r\n    pop = X.shape[0]  # 种群大小\r\n    fitness = np.zeros([pop, 1])  # 创建一个形状为 (pop, 1) 的全零数组 fitness，用于存储适应度值\r\n    for i in range(pop):\r\n        fitness[i] = fun(X[i, :])  # 计算第 i 个个体的适应度值，将结果赋值给 fitness[i]\r\n    return fitness\r\n\r\n'''适应度排序'''\r\ndef SortFitness(Fit):\r\n    fitness = np.sort(Fit, axis=0)  # 按列排序适应度值数组 Fit，将结果赋值给 fitness\r\n    index = np.argsort(Fit, axis=0)  # 返回按列排序后的索引数组，将结果赋值给 index\r\n    return fitness, index\r\n\r\n'''根据适应度对位置进行排序'''\r\ndef SortPosition(X, index):\r\n    Xnew = np.zeros(X.shape)  # 创建一个与位置数组 X 相同形状的全零数组 Xnew\r\n    for i in range(X.shape[0]):\r\n        Xnew[i, :] = X[index[i], :]  # 根据索引数组 index 对位置数组 X 进行排序，并将结果赋值给 Xnew\r\n    return Xnew\r\n\r\n\r\n# In[7]:\r\n\r\n\r\ndef DBO(pop, dim, lb, ub, MaxIter, fun):\r\n    # 参数设置\r\n    PballRolling = 0.2 # 滚球蜣螂比例\r\n    PbroodBall = 0.4 #产卵蜣螂比例\r\n    PSmall = 0.2 # 小蜣螂比例\r\n    Pthief = 0.2 # 偷窃蜣螂比例\r\n    BallRollingNum = int(pop*PballRolling) #滚球蜣螂数量\r\n    BroodBallNum = int(pop*PbroodBall) #产卵蜣螂数量\r\n    SmallNum = int(pop*PSmall) #小蜣螂数量\r\n    ThiefNum = int(pop*Pthief) #偷窃蜣螂数量\r\n    X, lb, ub = initial(pop, dim, ub, lb)  # 初始化种群\r\n    fitness = CaculateFitness(X, fun)  # 计算适应度值\r\n    # 记录全局最优\r\n    minIndex = np.argmin(fitness)  # 找到适应度最小值的索引\r\n    GbestScore = copy.copy(fitness[minIndex])  # 复制最小适应度值作为全局最优分数\r\n    GbestPositon = np.zeros([1, dim])  # 创建一个全0矩阵来存储全局最优位置\r\n    GbestPositon[0, :] = copy.copy(X[minIndex, :])  # 复制最小适应度对应的位置为全局最优位置\r\n    Curve = np.zeros([MaxIter, 1])  # 创建一个全0数组用于记录迭代过程中的最优分数\r\n    Xl = copy.deepcopy(X)  # 用于记录上一代的种群位置\r\n\r\n    # 记录当前代种群\r\n    cX = copy.deepcopy(X)  # 复制当前种群位置\r\n    cFit = copy.deepcopy(fitness)  # 复制当前种群适应度值\r\n    for t in range(MaxIter):   # 迭代次数循环，从0到MaxIter-1\r\n        print(\"第\" + str(t) + \"次迭代\")\r\n        # 蜣螂滚动 文献中式（1），（2）更新位置\r\n        # 获取种群最差值\r\n        maxIndex = np.argmax(fitness)  # 找到适应度最大值的索引\r\n        Wort = copy.copy(X[maxIndex, :])  # 复制最大适应度对应的位置为Wort\r\n        r2 = np.random.random()  # 生成一个随机数r2\r\n        for i in range(0,BallRollingNum): \r\n            # 循环迭代变量 i 在从 0 到 BallRollingNum（不包括）的范围内\r\n            if r2<0.9: # 如果 r2 的值小于 0.9\r\n                if np.random.random()>0.5:  # 如果 np.random.random() 生成的随机数大于 0.5\r\n                    alpha=1  # 设置alpha为1\r\n                else:  # 如果 np.random.random() 生成的随机数不大于 0.5\r\n                    alpha=-1 # 设置alpha为-1\r\n                b = 0.3  # 设置b为0.3\r\n                k = 0.1  # 设置k为0.1\r\n                X[i,:]=cX[i,:]+b*np.abs(cX[i,:]-Wort)+alpha*k*Xl[i,:]\r\n                # 对 X 数组中的第 i 行进行赋值计算，计算结果由 cX[i, :], b, np.abs(cX[i, :] - Wort), alpha 和 k * Xl[i, :] 组合得到\r\n            else:  # 如果 r2 的值不小于 0.9\r\n                theta = np.random.randint(180)# 生成一个0到179之间的随机整数\r\n                if theta==0 or theta == 90 or theta == 180: # 如果theta的值为0、90或180度\r\n                    X[i,:]=copy.copy(cX[i,:])# 将当前位置复制给新位置\r\n                else:\r\n                    theta = theta*np.pi/180 # 将theta转换为弧度制\r\n                    X[i,:]=cX[i,:]+np.tan(theta)*np.abs(cX[i,:]-Xl[i,:])\r\n                    # 对 X 数组中的第 i 行进行赋值计算，计算结果由 cX[i, :], np.tan(theta), np.abs(cX[i, :] - Xl[i, :]) 组合得到\r\n            for j in range(dim): # 循环迭代变量 j 在从 0 到 dim 的范围内\r\n                if X[i,j]>ub[j]:  # 如果 X 数组中的第 i 行、第 j 列的值大于上界 ub[j]\r\n                    X[i,j]=ub[j]  # 将 X 数组中的第 i 行、第 j 列的值设置为上界 ub[j]\r\n                if X[i,j]<lb[j]:  # 如果 X 数组中的第 i 行、第 j 列的值小于下界 lb[j]\r\n                    X[i,j]=lb[j]  # 将 X 数组中的第 i 行、第 j 列的值设置为下界 lb[j]\r\n            fitness[i]=fun(X[i,:]) # 计算第 i 行的适应度值，计算结果由 fun 函数根据 X[i, :] 得到\r\n            if fitness[i]<GbestScore:  # 如果第 i 行的适应度值小于全局最优适应度值 GbestScore\r\n                GbestScore=copy.copy(fitness[i]) # 将全局最优适应度值 GbestScore 更新为第 i 行的适应度值的副本\r\n                GbestPositon[0,:]=copy.copy(X[i,:])  # 将全局最优位置 GbestPositon 的第一行设置为 X 数组中第 i 行的副本\r\n        # 当前迭代最优\r\n        minIndex=np.argmin(fitness)  # 使用 np.argmin 函数找到适应度值数组 fitness 中的最小值的索引\r\n        GbestB = copy.copy(X[minIndex,:])  # 将 GbestB 设置为 X 数组中索引为 minIndex 的行的副本\r\n        # 蜣螂产卵 ，文献中式（3）\r\n        R=1-t/MaxIter # 根据当前迭代次数 t 和最大迭代次数 MaxIter 计算 R 的值\r\n        X1=GbestB*(1-R)  # 根据全局最优解 GbestB 和 R 计算 X1 的值\r\n        X2=GbestB*(1+R)  # 根据全局最优解 GbestB 和 R 计算 X2 的值\r\n        Lb = np.zeros(dim)  # 创建长度为 dim 的零数组 Lb 和 Ub\r\n        Ub = np.zeros(dim)\r\n        for j in range(dim):  # 循环迭代变量 j 在从 0 到 dim 的范围内\r\n            Lb[j]=max(X1[j],lb[j])  # Lb[j] 的值为 X1[j] 和 lb[j] 中的较大值\r\n            Ub[j]=min(X2[j],ub[j])  # Ub[j] 的值为 X2[j] 和 ub[j] 中的较小值\r\n        for i in range(BallRollingNum,BallRollingNum+BroodBallNum):\r\n            # 循环迭代变量 i 在从 BallRollingNum 到 BallRollingNum + BroodBallNum 的范围内\r\n            b1=np.random.random()  # 生成一个随机数 b1\r\n            b2=np.random.random()  # 生成一个随机数 b2\r\n            X[i,:]=GbestB+b1*(cX[i,:]-Lb)+b2*(cX[i,:]-Ub)# 根据公式更新 X 数组中的第 i 行的值\r\n            for j in range(dim): # 循环迭代变量 j 在从 0 到 dim 的范围内\r\n                if X[i,j]>ub[j]:  # 如果 X 数组中的第 i 行、第 j 列的值大于上界 ub[j]\r\n                    X[i,j]=ub[j]  # 将 X 数组中的第 i 行、第 j 列的值设置为上界 ub[j]\r\n                if X[i,j]<lb[j]:  # 如果 X 数组中的第 i 行、第 j 列的值小于下界 lb[j]\r\n                    X[i,j]=lb[j]  # 将 X 数组中的第 i 行、第 j 列的值设置为下界 lb[j]\r\n            fitness[i]=fun(X[i,:])  # 计算第 i 行的适应度值，计算结果由 fun 函数根据 X[i, :] 得到\r\n            if fitness[i]<GbestScore:  # 如果第 i 行的适应度值小于全局最优适应度值 GbestScore\r\n                GbestScore=copy.copy(fitness[i]) # 将全局最优适应度值 GbestScore 更新为第 i 行的适应度值的副本\r\n                GbestPositon[0,:]=copy.copy(X[i,:]) # 将全局最优位置 GbestPositon 的第一行设置为 X 数组中第 i 行的副本\r\n        # 小蜣螂更新\r\n        #文献中(5),(6)\r\n        R=1-t/MaxIter # 根据当前迭代次数 t 和最大迭代次数 MaxIter 计算 R 的值\r\n        X1=GbestPositon[0,:]*(1-R) # 根据全局最优位置 GbestPositon 和 R 计算 X1 的值\r\n        X2=GbestPositon[0,:]*(1+R) # 根据全局最优位置 GbestPositon 和 R 计算 X2 的值\r\n        Lb = np.zeros(dim) # 创建长度为 dim 的零数组 Lb 和 Ub\r\n        Ub = np.zeros(dim)\r\n        for j in range(dim):  # 循环迭代变量 j 在从 0 到 dim 的范围内\r\n            Lb[j]=max(X1[j],lb[j])  # Lb[j] 的值为 X1[j] 和 lb[j] 中的较大值\r\n            Ub[j]=min(X2[j],ub[j])  # Ub[j] 的值为 X2[j] 和 ub[j] 中的较小值\r\n        for i in range(BallRollingNum+BroodBallNum,BallRollingNum+BroodBallNum+SmallNum):  \r\n            # 循环迭代变量 i 在从 BallRollingNum + BroodBallNum 到 BallRollingNum + BroodBallNum + SmallNum 的范围内\r\n            C1 = np.random.random([1,dim]) # 生成一个随机数组 C1，形状为 [1, dim]\r\n            C2 = np.random.random([1,dim]) # 生成一个随机数组 C2，形状为 [1, dim]\r\n            X[i,:]=GbestPositon[0,:]+C1*(cX[i,:]-Lb)+C2*(cX[i,:]-Ub) # 根据公式更新 X 数组中的第 i 行的值\r\n            for j in range(dim): # 循环迭代变量 j 在从 0 到 dim 的范围内\r\n                if X[i,j]>ub[j]: # 如果 X 数组中的第 i 行、第 j 列的值大于上界 ub[j]\r\n                    X[i,j]=ub[j] # 将 X 数组中的第 i 行、第 j 列的值设置为上界 ub[j]\r\n                if X[i,j]<lb[j]: # 如果 X 数组中的第 i 行、第 j 列的值小于下界 lb[j]\r\n                    X[i,j]=lb[j] # 将 X 数组中的第 i 行、第 j 列的值设置为下界 lb[j]\r\n            fitness[i]=fun(X[i,:]) # 计算第 i 行的适应度值，计算结果由 fun 函数根据 X[i, :] 得到\r\n            if fitness[i]<GbestScore: # 如果第 i 行的适应度值小于全局最优适应度值 GbestScore\r\n                GbestScore=copy.copy(fitness[i]) # 将全局最优适应度值 GbestScore 更新为第 i 行的适应度值的副本\r\n                GbestPositon[0,:]=copy.copy(X[i,:]) # 将全局最优位置 GbestPositon 的第一行设置为 X 数组中第 i 行的副本\r\n        # 当前迭代最优\r\n        minIndex=np.argmin(fitness) # 使用 np.argmin 函数找到适应度值数组 fitness 中的最小值的索引\r\n        GbestB = copy.copy(X[minIndex,:]) # 将 GbestB 设置为 X 数组中索引为 minIndex 的行的副本\r\n        # 偷窃蜣螂更新 \r\n        # 文献中式（7）\r\n        for i in range(pop-ThiefNum,pop): # 循环迭代变量 i 在从 pop - ThiefNum 到 pop 的范围内\r\n            g=np.random.randn() # 生成一个符合标准正态分布的随机数 g\r\n            S=0.5 # 设置 S 的值为 0.5\r\n            X[i,:]=GbestPositon[0,:]+g*S*(np.abs(cX[i,:]-GbestB)+np.abs(cX[i,:]-GbestPositon[0,:])) # 根据公式更新 X 数组中的第 i 行的值\r\n            for j in range(dim): # 循环迭代变量 j 在从 0 到 dim 的范围内\r\n                if X[i,j]>ub[j]: # 如果 X 数组中的第 i 行、第 j 列的值大于上界 ub[j]\r\n                    X[i,j]=ub[j] # 将 X 数组中的第 i 行、第 j 列的值设置为上界 ub[j]\r\n                if X[i,j]<lb[j]: # 如果 X 数组中的第 i 行、第 j 列的值小于下界 lb[j]\r\n                    X[i,j]=lb[j] # 将 X 数组中的第 i 行、第 j 列的值设置为下界 lb[j]\r\n            fitness[i]=fun(X[i,:]) # 计算第 i 行的适应度值，计算结果由 fun 函数根据 X[i, :] 得到\r\n            if fitness[i]<GbestScore: # 如果第 i 行的适应度值小于全局最优适应度值 GbestScore\r\n                GbestScore=copy.copy(fitness[i]) # 将全局最优适应度值 GbestScore 更新为第 i 行的适应度值的副本\r\n                GbestPositon[0,:]=copy.copy(X[i,:])  # 将全局最优位置 GbestPositon 的第一行设置为 X 数组中第 i 行的副本\r\n        # 记录t代种群\r\n        Xl= copy.deepcopy(cX) # 将 Xl 设置为 cX 的深拷贝，即创建一个与 cX 一样的副本\r\n        #更新当前代种群\r\n        for i in range(pop):  # 循环迭代变量 i 在从 0 到 pop 的范围内\r\n            if fitness[i]<cFit[i]:  # 如果第 i 个个体的适应度值小于当前代个体的适应度值 cFit[i]\r\n                cFit[i]=copy.copy(fitness[i]) # 将当前代个体的适应度值 cFit[i] 更新为第 i 个个体的适应度值的副本\r\n                cX[i,:]=copy.copy(X[i,:])  # 将当前代个体 cX 的第 i 行设置为 X 数组中第 i 行的副本\r\n        \r\n        Curve[t] = GbestScore # 将 Curve 数组的第 t 个元素设置为全局最优适应度值 GbestScore\r\n\r\n    return GbestScore, GbestPositon, Curve # 返回全局最优适应度值 GbestScore、全局最优位置 GbestPositon 和适应度曲线 Curve\r\n\r\n\r\n# In[8]:\r\n\r\n\r\ndef ceemdan_decompose(series=None, trials=10, num_clusters=3): \r\n    decom = CEEMDAN()  # 创建CEEMDAN对象\r\n    decom.trials = trials  # 设置分解的试验次数\r\n    df_ceemdan = pd.DataFrame(decom(series.values).T)  # 对数据进行CEEMDAN分解并转换为数据框\r\n    df_ceemdan.columns = ['imf'+str(i) for i in range(len(df_ceemdan.columns))]  # 为每一列设置列名为'imf' + 对应的索引号\r\n    return df_ceemdan  # 返回分解后的数据框\r\n\r\n\r\n# In[9]:\r\n\r\n\r\ndef sample_entropy(df_ceemdan=None, mm=1, r=0.1):\r\n    np_sampen = []  # 存储样本熵的列表\r\n    for i in range(len(df_ceemdan.columns)):\r\n        sample_entropy = sampen2(list(df_ceemdan['imf'+str(i)].values), mm=mm, r=r, normalize=True)  # 计算样本熵\r\n        np_sampen.append(sample_entropy[1][1])  # 将样本熵的值添加到列表中\r\n    df_sampen = pd.DataFrame(np_sampen, index=['imf'+str(i) for i in range(len(df_ceemdan.columns))])  # 创建样本熵的数据框，设置行索引为'imf' + 对应的索引号\r\n    return df_sampen  # 返回样本熵的数据框\r\n\r\n\r\n# In[10]:\r\n\r\n\r\ndef kmeans_cluster(df_sampen=None, num_clusters=3): \r\n    np_integrate_form = KMeans(n_clusters=num_clusters, random_state=9).fit_predict(df_sampen)  # 使用K均值聚类进行聚类操作\r\n    df_integrate_form = pd.DataFrame(np_integrate_form, index=['imf'+str(i) for i in range(len(df_sampen.index))], columns=['OT'])  # 创建聚类结果的数据框，设置行索引为'imf' + 对应的索引号，列名为'OT'\r\n    return df_integrate_form  # 返回聚类结果的数据框\r\n\r\n\r\n# In[11]:\r\n\r\n\r\ndef integrate_imfs(df_integrate_form=None, df_ceemdan=None): \r\n    df_tmp = pd.DataFrame()  # 创建一个空的数据框用于存储临时结果\r\n    for i in range(df_integrate_form.values.max()+1):\r\n        df_tmp['imf'+str(i)] = df_ceemdan[df_integrate_form[(df_integrate_form['OT']==i)].index].sum(axis=1)  # 对每个聚类簇内的IMF分量进行求和，得到综合的IMF分量\r\n        \r\n    df_integrate_result = df_tmp.T  # 对临时结果进行转置\r\n    df_integrate_result['sampen'] = sample_entropy(df_tmp).values  # 计算综合的IMF分量的样本熵，并将其作为新的列添加到结果数据框中\r\n    df_integrate_result.sort_values(by=['sampen'], ascending=False, inplace=True)  # 根据样本熵降序排列综合的IMF分量\r\n    df_integrate_result.index = ['co-imf'+str(i) for i in range(df_integrate_form.values.max()+1)]  # 为综合的IMF分量设置新的行索引，命名规则为'co-imf' + 对应的索引号\r\n    df_integrate_result = df_integrate_result.drop('sampen', axis=1, inplace=False)  # 移除样本熵这一列\r\n    return df_integrate_result.T  # 返回结果数据框的转置\r\n\r\n\r\n# In[12]:\r\n\r\n\r\ndef vmd_decompose(series=None, draw=True):\r\n    def training(X):\r\n        alpha = int(X[0])  # 将X的第一个元素转换为整数，表示alpha参数\r\n        K = int(X[1])  # 将X的第二个元素转换为整数，表示K参数\r\n        print(X)  # 输出X的值\r\n        u, u_hat, omega = VMD(series, alpha=alpha, tau=tau, K=K, DC=DC, init=init, tol=tol)  # 使用VMD函数对series进行信号分解\r\n        vmd = pd.DataFrame(u.T)  # 将分解后的结果转换为DataFrame对象\r\n        vmd.columns = ['imf'+str(i) for i in range(K)]  # 设置DataFrame的列名为'imf0', 'imf1', ...\r\n\r\n        np_sampen = []\r\n        for i in range(len(vmd.columns)):\r\n            SE = sampen2(list(vmd['imf'+str(i)].values), mm=1, r=0.1, normalize=True)  # 计算每个IMF的样本熵\r\n            np_sampen.append(SE[1][1])\r\n        df_sampen = pd.DataFrame(np_sampen, index=['imf'+str(i) for i in range(len(vmd.columns))])  # 将样本熵存储为DataFrame\r\n        sampen = df_sampen[0].mean()  # 计算平均样本熵\r\n        print(sampen)  # 输出平均样本熵\r\n        return sampen\r\n    \r\n    ub = np.array([4000, 15])  # 设置参数的上界\r\n    lb = np.array([100, 3])  # 设置参数的下界\r\n    pop = 1  # 种群大小\r\n    MaxIter = 1  # 最大迭代次数\r\n    dim = 2  # 参数维度\r\n    \r\n    # 主函数\r\n    GbestScore, GbestPositon, Curve = DBO(pop, dim, lb, ub, MaxIter, training)  # 使用DBO算法进行参数优化\r\n    print('最优适应度值：', GbestScore)  # 输出最优适应度值\r\n    print('最优解：', GbestPositon)  # 输出最优解\r\n    \r\n    # 绘制适应度曲线\r\n    plt.figure(1)  # 创建图形窗口\r\n    plt.semilogy(Curve, 'r-', linewidth=2)  # 绘制适应度曲线\r\n    plt.xlabel('Iteration', fontsize='medium')  # 设置x轴标签\r\n    plt.ylabel(\"Fitness\", fontsize='medium')  # 设置y轴标签\r\n    plt.grid()  # 添加网格线\r\n    plt.title('DBO', fontsize='large')  # 设置标题\r\n    plt.show()  # 显示图形\r\n    \r\n    imfs_vmd, imfs_hat, omega = VMD(series, alpha=int(GbestPositon[0]), tau=tau, K=int(GbestPositon[1]), DC=DC, init=init, tol=tol)  # 使用优化后的参数进行信号分解\r\n    df_vmd = pd.DataFrame(imfs_vmd.T)  # 将分解后的结果转换为DataFrame对象\r\n    df_vmd.columns = ['imf'+str(i) for i in range(int(GbestPositon[1]))]  # 设置DataFrame的列名为'imf0', 'imf1', ...\r\n    \r\n    return df_vmd  # 返回分解结果的DataFrame对象\r\n\r\n\r\n# In[13]:\r\n\r\n\r\ndef create_train_test_set(data=None, timestep=timestep, co_imf_predict_for_fitting=None):\r\n    if isinstance(data, pd.DataFrame):\r\n        dataY = data['sum'].values.reshape(-1, 1)  # 提取DataFrame中的目标变量列，并将其转换为二维数组\r\n        dataX = data.drop('sum', axis=1, inplace=False)  # 去除DataFrame中的目标变量列，得到特征变量列\r\n    else:\r\n        dataY = data.values.reshape(-1, 1)  # 将一维数组转换为二维数组作为目标变量\r\n        dataX = dataY  # 特征变量和目标变量相同\r\n\r\n    scalarX = MinMaxScaler(feature_range=(0, 1))  # 创建MinMaxScaler对象，用于特征变量归一化\r\n    dataX = scalarX.fit_transform(dataX)  # 对特征变量进行归一化\r\n    if co_imf_predict_for_fitting is not None:\r\n        co_imf_predict_for_fitting = scalarX.transform(co_imf_predict_for_fitting)  # 对要预测的特征变量进行归一化\r\n\r\n    scalarY = MinMaxScaler(feature_range=(0, 1))  # 创建MinMaxScaler对象，用于目标变量归一化\r\n    dataY = scalarY.fit_transform(dataY)  # 对目标变量进行归一化\r\n\r\n    trainX, trainY = [], []\r\n    for i in range(len(dataY) - timestep):\r\n        trainX.append(np.array(dataX[i:(i + timestep)]))  # 构建训练样本的特征序列\r\n        trainY.append(np.array(dataY[i + timestep]))  # 构建训练样本的目标值\r\n        if co_imf_predict_for_fitting is not None:\r\n            if i < (len(dataY) - timestep - len(co_imf_predict_for_fitting)):\r\n                trainX[i] = np.insert(trainX[i], timestep, dataX[i + timestep], 0)\r\n            else:\r\n                trainX[i] = np.insert(trainX[i], timestep, co_imf_predict_for_fitting[i - (len(dataY) - timestep - len(co_imf_predict_for_fitting))], 0)\r\n                # 在训练样本的特征序列末尾插入要预测的特征变量值\r\n\r\n    return np.array(trainX), np.array(trainY), scalarY  # 返回训练样本的特征序列、目标值和目标值的归一化器对象\r\n\r\n\r\n# In[14]:\r\n\r\n\r\ndef evaluation_model(y_test, y_pred):\r\n    y_test, y_pred = np.array(y_test).ravel(), np.array(y_pred).ravel()  # 将y_test和y_pred转换为一维数组\r\n    r2 = r2_score(y_test, y_pred)  # 计算R^2分数\r\n    rmse = mean_squared_error(y_test, y_pred, squared=False)  # 计算均方根误差（RMSE）\r\n    mae = mean_absolute_error(y_test, y_pred)  # 计算平均绝对误差（MAE）\r\n    mape = mean_absolute_percentage_error(y_test, y_pred)  # 计算平均绝对百分比误差（MAPE）\r\n    df_evaluation = pd.DataFrame({'r2': r2, 'rmse': rmse, 'mae': mae, 'mape': mape}, index=range(1))  # 创建评估结果的DataFrame\r\n    return df_evaluation  # 返回评估结果的DataFrame对象\r\n\r\n\r\n# In[15]:\r\n\r\n\r\ndef informer_predict(data=None, predict_duration=len(test), fitting=None, scalarY=None):\r\n    lr = 0.0001\r\n    epochs = 4\r\n    batch_size = 32\r\n    seq_len = 96\r\n    label_len = 48\r\n    pred_len = 24\r\n    rootpath = \"./\"\r\n    trainrate = 0.7\r\n\r\n    def training(X):\r\n        lr=X[0]\r\n        epochs=X[1]\r\n        batch_size=X[2]\r\n        # writer = SummaryWriter(rootpath + \"log/tensorboard/\")\r\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n        set_seed(0)\r\n\r\n        df = pd.read_csv(rootpath + \"data/ETT/ETTh1.csv\")\r\n        train = df.iloc[: int(trainrate * len(df)), :]\r\n        test = df.iloc[int(trainrate * len(df)):, :]\r\n\r\n        scaler = StandardScaler()\r\n        scaler.fit(train.iloc[:, 1:].values)\r\n\r\n        trainset = MyDataset(train, scaler, seq_len=96, label_len=48, pred_len=24)\r\n        trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\r\n\r\n        testset = MyDataset(test, scaler, seq_len=96, label_len=48, pred_len=24)\r\n        testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)\r\n\r\n        model = Informer().to(device)\r\n        criterion = nn.MSELoss()\r\n        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-3)\r\n\r\n        # show\r\n        # print(\"show...\")\r\n        # for (batch_x, batch_y, batch_x_mark, batch_y_mark) in tqdm(trainloader):\r\n        #     batch_x = batch_x.float().to(device)\r\n        #     batch_y = batch_y.float()\r\n        #     batch_x_mark = batch_x_mark.float().to(device)\r\n        #     batch_y_mark = batch_y_mark.float().to(device)\r\n        #\r\n        #     dec_inp = torch.zeros([batch_y.shape[0], pred_len, batch_y.shape[-1]]).float()\r\n        #     dec_inp = (\r\n        #         torch.cat([batch_y[:, :label_len, :], dec_inp], dim=1).float().to(device)\r\n        #     )\r\n        #     with writer as w:\r\n        #         w.add_graph(model, (batch_x, batch_x_mark, dec_inp, batch_y_mark))\r\n        #     break\r\n\r\n        # train\r\n        print(\"train...\")\r\n        model.train()\r\n        for e in range(epochs):\r\n            losses = []\r\n            for (batch_x, batch_y, batch_x_mark, batch_y_mark) in tqdm(trainloader):\r\n                optimizer.zero_grad()\r\n                batch_x = batch_x.float().to(device)\r\n                batch_y = batch_y.float()\r\n                batch_x_mark = batch_x_mark.float().to(device)\r\n                batch_y_mark = batch_y_mark.float().to(device)\r\n\r\n                dec_inp = torch.zeros([batch_y.shape[0], pred_len, batch_y.shape[-1]]).float()\r\n                dec_inp = torch.cat([batch_y[:, :label_len, :], dec_inp], dim=1).float().to(device)\r\n\r\n                pred = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\r\n                pred = pred[:, -pred_len:, :].to(device)\r\n                true = batch_y[:, -pred_len:, :].to(device)\r\n\r\n                loss = criterion(pred, true)\r\n                losses.append(loss.item())\r\n\r\n                loss.backward()\r\n                optimizer.step()\r\n\r\n            print(\"Epochs:\", e, \" || train loss: %.4f\" % np.mean(losses))\r\n\r\n        torch.save(model, rootpath + \"log/informer.pkl\")\r\n\r\n        # test\r\n        print(\"test...\")\r\n        # model = torch.load(\"./Informer/log/informer.pkl\").to(device)\r\n\r\n        model.eval()\r\n        losses = []\r\n        trues, preds = [], []\r\n        for (batch_x, batch_y, batch_x_mark, batch_y_mark) in tqdm(testloader):\r\n            batch_x = batch_x.float().to(device)\r\n            batch_y = batch_y.float()\r\n            batch_x_mark = batch_x_mark.float().to(device)\r\n            batch_y_mark = batch_y_mark.float().to(device)\r\n\r\n            dec_inp = torch.zeros([batch_y.shape[0], pred_len, batch_y.shape[-1]]).float()\r\n            dec_inp = torch.cat([batch_y[:, :label_len, :], dec_inp], dim=1).float().to(device)\r\n\r\n            pred = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\r\n\r\n            preds.extend(pred.detach().cpu().numpy())\r\n            trues.extend(batch_y.detach().cpu().numpy())\r\n\r\n            pred = pred[:, -pred_len:, :].to(device)\r\n            true = batch_y[:, -pred_len:, :].to(device)\r\n\r\n            loss = criterion(pred, true)\r\n            losses.append(loss.item())\r\n        print(\"test loss: %.4f\" % np.mean(losses))\r\n\r\n        temp_mse = mean_squared_error(pred, true)  # 计算均方误差\r\n        print(temp_mse)\r\n        return temp_mse\r\n\r\n        # np.save(rootpath + \"log/preds\", np.array(preds))\r\n        # np.save(rootpath + \"log/tures\", np.array(trues))\r\n        #\r\n        # # show\r\n        # pred = np.load(rootpath + \"log/preds.npy\")\r\n        # true = np.load(rootpath + \"log/tures.npy\")\r\n        #\r\n        # print(pred.shape, true.shape)\r\n        # plt.plot(pred[0, -24:, -1], label=\"pred\")\r\n        # plt.plot(true[0, -24:, -1], label=\"true\")\r\n        # plt.legend()\r\n        # plt.savefig(rootpath + \"img/show.png\")\r\n        # plt.show()\r\n\r\n    #优化参数\r\n    # lr = 0.0001\r\n    # epochs = 4\r\n    # batch_size = 32\r\n\r\n    ub = np.array([0.001, 10, 64])  # 优化算法上界\r\n    lb = np.array([0.00001, 1, 1])  # 优化算法下界\r\n    pop = 5  # 种群大小\r\n    MaxIter = 10  # 最大迭代次数\r\n    dim = 3  # 优化变量维度\r\n    GbestScore, GbestPositon, Curve = DBO(pop, dim, lb, ub, MaxIter, training)  # 使用Differential Evolution进行优化\r\n    print('最优适应度值：', GbestScore)\r\n    print('最优解：', GbestPositon)\r\n\r\n    GbestPositon = GbestPositon[0]\r\n    lr = int(GbestPositon[0])\r\n    epochs = int(GbestPositon[1])\r\n    batch_size = int(GbestPositon[2])\r\n    seq_len = 96\r\n    label_len = 48\r\n    pred_len = 24\r\n    rootpath = \"./\"\r\n    trainrate = 0.7\r\n\r\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n    set_seed(0)\r\n\r\n    df = pd.read_csv(rootpath + \"data/ETT/ETTh1.csv\")\r\n    train = df.iloc[: int(trainrate * len(df)), :]\r\n    test = df.iloc[int(trainrate * len(df)):, :]\r\n\r\n    scaler = StandardScaler()\r\n    scaler.fit(train.iloc[:, 1:].values)\r\n\r\n    trainset = MyDataset(train, scaler, seq_len=96, label_len=48, pred_len=24)\r\n    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\r\n\r\n    testset = MyDataset(test, scaler, seq_len=96, label_len=48, pred_len=24)\r\n    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)\r\n\r\n    model = Informer().to(device)\r\n    criterion = nn.MSELoss()\r\n    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-3)\r\n\r\n    # show\r\n    # print(\"show...\")\r\n    # for (batch_x, batch_y, batch_x_mark, batch_y_mark) in tqdm(trainloader):\r\n    #     batch_x = batch_x.float().to(device)\r\n    #     batch_y = batch_y.float()\r\n    #     batch_x_mark = batch_x_mark.float().to(device)\r\n    #     batch_y_mark = batch_y_mark.float().to(device)\r\n    #\r\n    #     dec_inp = torch.zeros([batch_y.shape[0], pred_len, batch_y.shape[-1]]).float()\r\n    #     dec_inp = (\r\n    #         torch.cat([batch_y[:, :label_len, :], dec_inp], dim=1).float().to(device)\r\n    #     )\r\n    #     with writer as w:\r\n    #         w.add_graph(model, (batch_x, batch_x_mark, dec_inp, batch_y_mark))\r\n    #     break\r\n\r\n    # train\r\n    print(\"train...\")\r\n    model.train()\r\n    for e in range(epochs):\r\n        train_losses = []\r\n        for (batch_x, batch_y, batch_x_mark, batch_y_mark) in tqdm(trainloader):\r\n            optimizer.zero_grad()\r\n            batch_x = batch_x.float().to(device)\r\n            batch_y = batch_y.float()\r\n            batch_x_mark = batch_x_mark.float().to(device)\r\n            batch_y_mark = batch_y_mark.float().to(device)\r\n\r\n            dec_inp = torch.zeros([batch_y.shape[0], pred_len, batch_y.shape[-1]]).float()\r\n            dec_inp = torch.cat([batch_y[:, :label_len, :], dec_inp], dim=1).float().to(device)\r\n\r\n            pred = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\r\n            pred = pred[:, -pred_len:, :].to(device)\r\n            true = batch_y[:, -pred_len:, :].to(device)\r\n\r\n            loss = criterion(pred, true)\r\n            train_losses.append(loss.item())\r\n\r\n            loss.backward()\r\n            optimizer.step()\r\n\r\n        print(\"Epochs:\", e, \" || train loss: %.4f\" % np.mean(train_losses))\r\n\r\n    torch.save(model, rootpath + \"log/informer.pkl\")\r\n\r\n    # test\r\n    print(\"test...\")\r\n    # model = torch.load(\"./Informer/log/informer.pkl\").to(device)\r\n\r\n    model.eval()\r\n    test_losses = []\r\n    trues, preds = [], []\r\n    for (batch_x, batch_y, batch_x_mark, batch_y_mark) in tqdm(testloader):\r\n        batch_x = batch_x.float().to(device)\r\n        batch_y = batch_y.float()\r\n        batch_x_mark = batch_x_mark.float().to(device)\r\n        batch_y_mark = batch_y_mark.float().to(device)\r\n\r\n        dec_inp = torch.zeros([batch_y.shape[0], pred_len, batch_y.shape[-1]]).float()\r\n        dec_inp = torch.cat([batch_y[:, :label_len, :], dec_inp], dim=1).float().to(device)\r\n\r\n        pred = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\r\n\r\n        preds.extend(pred.detach().cpu().numpy())\r\n        trues.extend(batch_y.detach().cpu().numpy())\r\n\r\n        pred = pred[:, -pred_len:, :].to(device)\r\n        true = batch_y[:, -pred_len:, :].to(device)\r\n\r\n        loss = criterion(pred, true)\r\n        test_losses.append(loss.item())\r\n    print(\"test loss: %.4f\" % np.mean(test_losses))\r\n\r\n    temp_mse = mean_squared_error(pred, true)  # 计算均方误差\r\n    print(\"优化后的均方误差：\",temp_mse)\r\n\r\n    np.save(rootpath + \"log/preds\", np.array(preds))\r\n    np.save(rootpath + \"log/tures\", np.array(trues))\r\n\r\n    # show\r\n    pred = np.load(rootpath + \"log/preds.npy\")\r\n    true = np.load(rootpath + \"log/tures.npy\")\r\n\r\n    print(pred.shape, true.shape)\r\n    plt.plot(pred[0, -24:, -1], label=\"pred\")\r\n    plt.plot(true[0, -24:, -1], label=\"true\")\r\n    plt.legend()\r\n    plt.savefig(rootpath + \"img/show.png\")\r\n    plt.show()\r\n\r\n\r\n    df_gru_evaluation = evaluation_model(true, pred)  # 评估模型性能\r\n    y_test_predict = pred.ravel().reshape(-1, 1)\r\n\r\n    y_test_predict_result = scalarY.inverse_transform(y_test_predict)  # 将预测结果反归一化\r\n    y_test_raw = scalarY.inverse_transform(true)  # 将测试集目标值反归一化\r\n    df_predict_raw = pd.DataFrame({'raw': y_test_raw.ravel(), 'predict': y_test_predict_result.ravel()},\r\n                                  index=range(len(y_test_raw)))  # 创建预测结果的DataFrame\r\n    df_train_loss = pd.DataFrame({'loss': train_losses, 'val_loss': test_losses},\r\n                                 index=range(len(test_losses)))  # 创建训练损失的DataFrame\r\n\r\n    return df_predict_raw, df_gru_evaluation, df_train_loss\r\n\r\n\r\n# In[16]:\r\n\r\n\r\n# CEEMDAN 分解\r\ndf_ceemdan = ceemdan_decompose(series_close)  # 对 series_close 应用 CEEMDAN 分解，得到分解后的数据框 df_ceemdan\r\ndf_ceemdan.plot(title='CEEMDAN 分解', subplots=True, figsize=(8, 8))  # 绘制 CEEMDAN 分解结果的子图，设置标题和图像大小\r\n\r\n\r\n# In[17]:\r\n\r\n\r\n# 样本熵的计算\r\ndf_sampen = sample_entropy(df_ceemdan)  # 计算 CEEMDAN 分解结果的样本熵，保存在数据框 df_sampen 中\r\ndf_sampen.plot(title='SE')  # 绘制样本熵的图像，设置标题为 'SE'\r\n\r\n\r\n# In[18]:\r\n\r\n\r\n# 通过样本熵的K-Means聚类\r\ndf_integrate_form = kmeans_cluster(df_sampen)  # 对样本熵数据框 df_sampen 进行 K-Means 聚类，得到聚类结果保存在数据框 df_integrate_form 中\r\nprint(df_integrate_form)  # 打印聚类结果 df_integrate_form\r\n\r\n\r\n# In[19]:\r\n\r\n\r\n# 将IMF和残留物整合为3个共同IMF\r\ndf_integrate_result = integrate_imfs(df_integrate_form, df_ceemdan)  # 将聚类结果 df_integrate_form 和 CEEMDAN 分解结果 df_ceemdan 整合为3个共同IMF，保存在数据框 df_integrate_result 中\r\ndf_integrate_result.plot(title='Co-IMFs', subplots=True)  # 绘制共同IMFs的子图，设置标题为 'Co-IMFs'\r\n\r\n\r\n# In[20]:\r\n\r\n\r\n# 通过VMD分解高频的Co-IMF0\r\ndf_vmd_co_imf0 = vmd_decompose(df_integrate_result['co-imf0'])  # 使用 VMD 对高频的 Co-IMF0 进行分解，得到分解结果保存在数据框 df_vmd_co_imf0 中\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\ndf_vmd_co_imf0.plot(title='VMD 分解', subplots=True, figsize=(10, 8))  # 绘制 VMD 分解结果 df_vmd_co_imf0 的子图，设置标题为 'VMD 分解'，图形大小为 (10, 8)\r\n\r\n\r\n# In[21]:\r\n\r\n\r\ndf_vmd_co_imf0['sum'] = df_integrate_result['co-imf0']  # 将 df_integrate_result['co-imf0'] 列赋值给 df_vmd_co_imf0 的 'sum' 列\r\n\r\nco_imf0_predict_raw, co_imf0_gru_evaluation, co_imf0_train_loss = informer_predict(df_vmd_co_imf0)  # 使用 informer 进行预测并得到预测结果、评估结果和训练损失\r\n\r\nprint('======Co-IMF0 最终预测======\\n', co_imf0_gru_evaluation)  # 打印 Co-IMF0 的最终预测评估结果\r\n\r\nco_imf0_predict_raw.plot(title='Co-IMF0 预测结果')  # 绘制 Co-IMF0 的预测结果图，设置标题为 'Co-IMF0 预测结果'\r\n\r\nco_imf0_train_loss.plot(title='Co-IMF0 训练损失')  # 绘制 Co-IMF0 的训练损失图，设置标题为 'Co-IMF0 训练损失'\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\nco_imf1_predict_raw, co_imf1_gru_evaluation, co_imf1_train_loss = informer_predict(df_integrate_result['co-imf1'])  # 使用 LSTM 进行预测并得到预测结果、评估结果和训练损失\r\n\r\nprint('======Co-IMF1 最终预测======\\n', co_imf1_gru_evaluation)  # 打印 Co-IMF1 的最终预测评估结果\r\n\r\nco_imf1_predict_raw.plot(title='Co-IMF1 预测结果')  # 绘制 Co-IMF1 的预测结果图，设置标题为 'Co-IMF1 预测结果'\r\n\r\nco_imf1_train_loss.plot(title='Co-IMF1 训练损失')  # 绘制 Co-IMF1 的训练损失图，设置标题为 'Co-IMF1 训练损失'\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\nco_imf2_predict_raw, co_imf2_gru_evaluation, co_imf2_train_loss = informer_predict(df_integrate_result['co-imf2'])  # 使用 LSTM 进行预测并得到预测结果、评估结果和训练损失\r\n\r\nprint('======Co-IMF2 最终预测======\\n', co_imf2_gru_evaluation)  # 打印 Co-IMF2 的最终预测评估结果\r\n\r\nco_imf2_predict_raw.plot(title='Co-IMF2 预测结果')  # 绘制 Co-IMF2 的预测结果图，设置标题为 'Co-IMF2 预测结果'\r\n\r\nco_imf2_train_loss.plot(title='Co-IMF2 训练损失')  # 绘制 Co-IMF2 的训练损失图，设置标题为 'Co-IMF2 训练损失'\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\nresult = co_imf0_predict_raw['predict'] + co_imf1_predict_raw['predict'] + co_imf2_predict_raw['predict']  # 将 Co-IMF0、Co-IMF1 和 Co-IMF2 的预测结果相加得到最终预测结果\r\n\r\ndf_add_evaluation = evaluation_model(test, result)  # 对最终预测结果和真实值进行评估，得到评估结果\r\n\r\nprint('======最终预测======\\n', df_add_evaluation)  # 打印最终预测的评估结果\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\n# 创建一个图形窗口\r\nplt.figure(figsize=(12, 3))\r\n\r\n# 设置图形标题和字体大小\r\nplt.title('CEEMDAN-DBO-VMD-DBO-LSTM', size=15)\r\n\r\n# 绘制真实值曲线\r\nplt.plot(test, color='r', linewidth=2.5, linestyle=\"-\", label='Actual')\r\n\r\n# 绘制预测值曲线\r\nplt.plot(result, color='yellow', linewidth=2, linestyle=\"--\", label='Prediction')\r\n\r\n# 显示图例\r\nplt.legend()\r\n\r\n# 设置y轴标签和字体大小\r\nplt.ylabel('O3', size=15)\r\n\r\n# 设置x轴标签和字体大小\r\nplt.xlabel('time/day', size=15)\r\n\r\n# 显示图形\r\nplt.show()\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\n\r\n\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/CEEMDAN-DBO-VMD-DBO-Informer-00.py b/CEEMDAN-DBO-VMD-DBO-Informer-00.py
--- a/CEEMDAN-DBO-VMD-DBO-Informer-00.py	(revision 51c77cd249e8abb526c9844f9fe21e50fe7cbd33)
+++ b/CEEMDAN-DBO-VMD-DBO-Informer-00.py	(date 1697373454461)
@@ -14,7 +14,7 @@
 import numpy as np                  # 导入numpy并使用别名"np"以方便使用
 import matplotlib.pyplot as plt    # 导入matplotlib库进行数据可视化操作
 from PyEMD import CEEMDAN           # 从PyEMD库导入CEEMDAN模块
-from pyroapi import optim
+from pyroapi import optim, pyro
 from sampen import sampen2          # 从sampen库导入sampen2模块
 from vmdpy import VMD               # 从vmdpy库导入VMD模块
 import tensorflow as tf             # 导入tensorflow库进行机器学习和深度学习
@@ -32,7 +32,6 @@
 import numpy as np
 from sklearn.preprocessing import StandardScaler
 from tqdm import tqdm
-
 from data.dataset import MyDataset
 from model.informer import Informer
 from utils.setseed import set_seed
@@ -50,10 +49,11 @@
 
 
 # In[4]:
-
+rootpath = "./"
+# rootpath="/kaggle/input/dbo-inf/"
 
 # df_raw_data = pd.read_csv('焦作.csv', usecols=[0, 1])  # 从名为'焦作.csv'的CSV文件中读取数据，只使用第一列和第二列的数据创建DataFrame对象
-df_raw_data = pd.read_csv("./data/ETT/ETTh1.csv")
+df_raw_data = pd.read_csv(rootpath+"data/ETT/ETTh1.csv")
 X = 'OT'  # 将字符串'AQI'赋值给变量X，表示使用该列作为特征
 #
 series_close = pd.Series(df_raw_data[X].values, index=df_raw_data['date'])  # 使用列名为X的数据创建Series对象，使用'Date'列作为索引
@@ -409,12 +409,13 @@
     label_len = 48
     pred_len = 24
     rootpath = "./"
+    # rootpath = "/kaggle/input/dbo-inf/"
     trainrate = 0.7
 
     def training(X):
         lr=X[0]
-        epochs=X[1]
-        batch_size=X[2]
+        epochs=int(X[1])
+        batch_size=int(X[2])
         # writer = SummaryWriter(rootpath + "log/tensorboard/")
         device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
         set_seed(0)
@@ -541,14 +542,15 @@
     print('最优适应度值：', GbestScore)
     print('最优解：', GbestPositon)
 
-    GbestPositon = GbestPositon[0]
-    lr = int(GbestPositon[0])
+    # GbestPositon = GbestPositon[0]
+    lr = GbestPositon[0]
     epochs = int(GbestPositon[1])
     batch_size = int(GbestPositon[2])
     seq_len = 96
     label_len = 48
     pred_len = 24
     rootpath = "./"
+    # rootpath = "/kaggle/input/dbo-inf/"
     trainrate = 0.7
 
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
@@ -711,21 +713,21 @@
 
 
 # 通过VMD分解高频的Co-IMF0
-df_vmd_co_imf0 = vmd_decompose(df_integrate_result['co-imf0'])  # 使用 VMD 对高频的 Co-IMF0 进行分解，得到分解结果保存在数据框 df_vmd_co_imf0 中
+# df_vmd_co_imf0 = vmd_decompose(df_integrate_result['co-imf0'])  # 使用 VMD 对高频的 Co-IMF0 进行分解，得到分解结果保存在数据框 df_vmd_co_imf0 中
 
 
 # In[ ]:
 
 
-df_vmd_co_imf0.plot(title='VMD 分解', subplots=True, figsize=(10, 8))  # 绘制 VMD 分解结果 df_vmd_co_imf0 的子图，设置标题为 'VMD 分解'，图形大小为 (10, 8)
+# df_vmd_co_imf0.plot(title='VMD 分解', subplots=True, figsize=(10, 8))  # 绘制 VMD 分解结果 df_vmd_co_imf0 的子图，设置标题为 'VMD 分解'，图形大小为 (10, 8)
 
 
 # In[21]:
 
 
-df_vmd_co_imf0['sum'] = df_integrate_result['co-imf0']  # 将 df_integrate_result['co-imf0'] 列赋值给 df_vmd_co_imf0 的 'sum' 列
+# df_vmd_co_imf0['sum'] = df_integrate_result['co-imf0']  # 将 df_integrate_result['co-imf0'] 列赋值给 df_vmd_co_imf0 的 'sum' 列
 
-co_imf0_predict_raw, co_imf0_gru_evaluation, co_imf0_train_loss = informer_predict(df_vmd_co_imf0)  # 使用 informer 进行预测并得到预测结果、评估结果和训练损失
+co_imf0_predict_raw, co_imf0_gru_evaluation, co_imf0_train_loss = informer_predict(df_integrate_result['co-imf0'])  # 使用 informer 进行预测并得到预测结果、评估结果和训练损失
 
 print('======Co-IMF0 最终预测======\n', co_imf0_gru_evaluation)  # 打印 Co-IMF0 的最终预测评估结果
 
Index: .idea/workspace.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<project version=\"4\">\r\n  <component name=\"AutoImportSettings\">\r\n    <option name=\"autoReloadType\" value=\"SELECTIVE\" />\r\n  </component>\r\n  <component name=\"ChangeListManager\">\r\n    <list default=\"true\" id=\"bbaf361f-ff6f-4b79-b493-b34da3b160dc\" name=\"Changes\" comment=\"\" />\r\n    <option name=\"SHOW_DIALOG\" value=\"false\" />\r\n    <option name=\"HIGHLIGHT_CONFLICTS\" value=\"true\" />\r\n    <option name=\"HIGHLIGHT_NON_ACTIVE_CHANGELIST\" value=\"false\" />\r\n    <option name=\"LAST_RESOLUTION\" value=\"IGNORE\" />\r\n  </component>\r\n  <component name=\"FlaskConsoleOptions\" custom-start-script=\"import sys&#10;sys.path.extend([WORKING_DIR_AND_PYTHON_PATHS])&#10;from flask.cli import ScriptInfo&#10;locals().update(ScriptInfo(create_app=None).load_app().make_shell_context())&#10;print(&quot;Python %s on %s\\nApp: %s [%s]\\nInstance: %s&quot; % (sys.version, sys.platform, app.import_name, app.env, app.instance_path))\">\r\n    <envs>\r\n      <env key=\"FLASK_APP\" value=\"app\" />\r\n    </envs>\r\n    <option name=\"myCustomStartScript\" value=\"import sys&#10;sys.path.extend([WORKING_DIR_AND_PYTHON_PATHS])&#10;from flask.cli import ScriptInfo&#10;locals().update(ScriptInfo(create_app=None).load_app().make_shell_context())&#10;print(&quot;Python %s on %s\\nApp: %s [%s]\\nInstance: %s&quot; % (sys.version, sys.platform, app.import_name, app.env, app.instance_path))\" />\r\n    <option name=\"myEnvs\">\r\n      <map>\r\n        <entry key=\"FLASK_APP\" value=\"app\" />\r\n      </map>\r\n    </option>\r\n  </component>\r\n  <component name=\"Git.Settings\">\r\n    <option name=\"RECENT_GIT_ROOT_PATH\" value=\"$PROJECT_DIR$\" />\r\n  </component>\r\n  <component name=\"MarkdownSettingsMigration\">\r\n    <option name=\"stateVersion\" value=\"1\" />\r\n  </component>\r\n  <component name=\"ProjectId\" id=\"2WmQUvKAT59ACvrwYhFAbpqkEfM\" />\r\n  <component name=\"ProjectLevelVcsManager\" settingsEditedManually=\"true\" />\r\n  <component name=\"ProjectViewState\">\r\n    <option name=\"hideEmptyMiddlePackages\" value=\"true\" />\r\n    <option name=\"showLibraryContents\" value=\"true\" />\r\n  </component>\r\n  <component name=\"PropertiesComponent\">{\r\n  &quot;keyToString&quot;: {\r\n    &quot;RunOnceActivity.OpenProjectViewOnStart&quot;: &quot;true&quot;,\r\n    &quot;RunOnceActivity.ShowReadmeOnStart&quot;: &quot;true&quot;,\r\n    &quot;WebServerToolWindowFactoryState&quot;: &quot;false&quot;,\r\n    &quot;last_opened_file_path&quot;: &quot;D:/soft/anaconda/workspace/Informer-1-master&quot;,\r\n    &quot;node.js.detected.package.eslint&quot;: &quot;true&quot;,\r\n    &quot;node.js.detected.package.tslint&quot;: &quot;true&quot;,\r\n    &quot;node.js.selected.package.eslint&quot;: &quot;(autodetect)&quot;,\r\n    &quot;node.js.selected.package.tslint&quot;: &quot;(autodetect)&quot;,\r\n    &quot;settings.editor.selected.configurable&quot;: &quot;com.jetbrains.python.configuration.PyActiveSdkModuleConfigurable&quot;,\r\n    &quot;vue.rearranger.settings.migration&quot;: &quot;true&quot;\r\n  }\r\n}</component>\r\n  <component name=\"RunManager\" selected=\"Python.CEEMDAN-DBO-VMD-DBO-Informer-00\">\r\n    <configuration name=\"CEEMDAN-DBO-VMD-DBO-Informer-00\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\r\n      <module name=\"Informer-1-master\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs>\r\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\r\n      </envs>\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/CEEMDAN-DBO-VMD-DBO-Informer-00.py\" />\r\n      <option name=\"PARAMETERS\" value=\"\" />\r\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\r\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\r\n      <option name=\"MODULE_MODE\" value=\"false\" />\r\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\r\n      <option name=\"INPUT_FILE\" value=\"\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <configuration name=\"main\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\r\n      <module name=\"Informer-1-master\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs>\r\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\r\n      </envs>\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/main.py\" />\r\n      <option name=\"PARAMETERS\" value=\"\" />\r\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\r\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\r\n      <option name=\"MODULE_MODE\" value=\"false\" />\r\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\r\n      <option name=\"INPUT_FILE\" value=\"\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <recent_temporary>\r\n      <list>\r\n        <item itemvalue=\"Python.CEEMDAN-DBO-VMD-DBO-Informer-00\" />\r\n        <item itemvalue=\"Python.main\" />\r\n      </list>\r\n    </recent_temporary>\r\n  </component>\r\n  <component name=\"SpellCheckerSettings\" RuntimeDictionaries=\"0\" Folders=\"0\" CustomDictionaries=\"0\" DefaultDictionary=\"application-level\" UseSingleDictionary=\"true\" transferred=\"true\" />\r\n  <component name=\"TaskManager\">\r\n    <task active=\"true\" id=\"Default\" summary=\"Default task\">\r\n      <changelist id=\"bbaf361f-ff6f-4b79-b493-b34da3b160dc\" name=\"Changes\" comment=\"\" />\r\n      <created>1697335297139</created>\r\n      <option name=\"number\" value=\"Default\" />\r\n      <option name=\"presentableId\" value=\"Default\" />\r\n      <updated>1697335297139</updated>\r\n      <workItem from=\"1697335298704\" duration=\"9266000\" />\r\n      <workItem from=\"1697352380062\" duration=\"1582000\" />\r\n      <workItem from=\"1697354258108\" duration=\"2681000\" />\r\n      <workItem from=\"1697360017868\" duration=\"524000\" />\r\n    </task>\r\n    <servers />\r\n  </component>\r\n  <component name=\"TypeScriptGeneratedFilesManager\">\r\n    <option name=\"version\" value=\"3\" />\r\n  </component>\r\n  <component name=\"Vcs.Log.Tabs.Properties\">\r\n    <option name=\"TAB_STATES\">\r\n      <map>\r\n        <entry key=\"MAIN\">\r\n          <value>\r\n            <State />\r\n          </value>\r\n        </entry>\r\n      </map>\r\n    </option>\r\n  </component>\r\n  <component name=\"com.intellij.coverage.CoverageDataManagerImpl\">\r\n    <SUITE FILE_PATH=\"coverage/Informer_1_master$CEEMDAN_DBO_VMD_DBO_Informer_00.coverage\" NAME=\"CEEMDAN-DBO-VMD-DBO-Informer-00 Coverage Results\" MODIFIED=\"1697358464386\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Informer_1_master$main.coverage\" NAME=\"main Coverage Results\" MODIFIED=\"1697338009327\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n  </component>\r\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/workspace.xml b/.idea/workspace.xml
--- a/.idea/workspace.xml	(revision 51c77cd249e8abb526c9844f9fe21e50fe7cbd33)
+++ b/.idea/workspace.xml	(date 1697439711755)
@@ -4,7 +4,9 @@
     <option name="autoReloadType" value="SELECTIVE" />
   </component>
   <component name="ChangeListManager">
-    <list default="true" id="bbaf361f-ff6f-4b79-b493-b34da3b160dc" name="Changes" comment="" />
+    <list default="true" id="bbaf361f-ff6f-4b79-b493-b34da3b160dc" name="Changes" comment="">
+      <change beforePath="$PROJECT_DIR$/CEEMDAN-DBO-VMD-DBO-Informer-00.py" beforeDir="false" afterPath="$PROJECT_DIR$/CEEMDAN-DBO-VMD-DBO-Informer-00.py" afterDir="false" />
+    </list>
     <option name="SHOW_DIALOG" value="false" />
     <option name="HIGHLIGHT_CONFLICTS" value="true" />
     <option name="HIGHLIGHT_NON_ACTIVE_CHANGELIST" value="false" />
@@ -111,6 +113,7 @@
       <workItem from="1697352380062" duration="1582000" />
       <workItem from="1697354258108" duration="2681000" />
       <workItem from="1697360017868" duration="524000" />
+      <workItem from="1697431010060" duration="2115000" />
     </task>
     <servers />
   </component>
